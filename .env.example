# LLM Provider selection
LLM_PROVIDER=gemini        # Options: gemini, ollama, openai, claude
LLM_MODEL=gemini-2.0-flash # Example: gemini-2.0-flash, tinyllama, gpt-4-turbo, claude-3-haiku

# API Keys for cloud providers
GEMINI_API_KEY=key_here
OPENAI_API_KEY=your-openai-api-key
ANTHROPIC_API_KEY=your-claude-api-key

# Ollama local provider config
OLLAMA_API=http://localhost:11434/api/generate
OLLAMA_MODEL=tinyllama     # Example: tinyllama, phi3, mistral

# Alert log and output paths
ALERT_LOG_PATH=/var/ossec/logs/alerts/alerts.json
ENRICHED_OUTPUT_PATH=llm_enriched_alerts.json

# Elasticsearch config
ELASTICSEARCH_URL=https://localhost:9200
ELASTICSEARCH_USE_SSL=false
ELASTIC_USER=admin
ELASTIC_PASS=admin
ENRICHED_INDEX=wazuh-enriched-alerts
ELASTIC_CA_BUNDLE=
 